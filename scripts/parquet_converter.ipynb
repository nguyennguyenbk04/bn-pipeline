{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f41a0ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T03:56:22.754032Z",
     "iopub.status.busy": "2025-07-09T03:56:22.753917Z",
     "iopub.status.idle": "2025-07-09T03:56:22.797473Z",
     "shell.execute_reply": "2025-07-09T03:56:22.796955Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "jar_dir = \"/home/bnguyen/Desktop/DE_project/scripts/jars\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    f\"--jars {jar_dir}/hadoop-azure-3.3.6.jar,\"\n",
    "    f\"{jar_dir}/azure-storage-8.6.6.jar,\"\n",
    "    f\"{jar_dir}/jetty-client-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-http-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-io-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-ajax-9.4.43.v20210629.jar \"\n",
    "    \"pyspark-shell\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4464281b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T03:56:22.799189Z",
     "iopub.status.busy": "2025-07-09T03:56:22.799066Z",
     "iopub.status.idle": "2025-07-09T03:56:36.254378Z",
     "shell.execute_reply": "2025-07-09T03:56:36.253935Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 22:01:06 WARN Utils: Your hostname, lenovo-slim resolves to a loopback address: 127.0.1.1; using 192.168.1.12 instead (on interface wlp2s0)\n",
      "25/07/14 22:01:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/14 22:01:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Init spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read from Azure Blob\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f928f32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T03:56:36.256220Z",
     "iopub.status.busy": "2025-07-09T03:56:36.256073Z",
     "iopub.status.idle": "2025-07-09T03:56:36.401259Z",
     "shell.execute_reply": "2025-07-09T03:56:36.400862Z"
    }
   },
   "outputs": [],
   "source": [
    "# Landing zone access key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.kho.blob.core.windows.net\",\n",
    "    \"DV9ioy7PXW5gTHcotPx6/ILeKjue7zHgGoNYkt6dvPjqK5XyfK4DVXzfFxmIVGWNj6GVU8Q3a0xl+AStBjZBqg==\"\n",
    ")\n",
    "\n",
    "# Bronze access key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mybronze.dfs.core.windows.net\",\n",
    "    \"c5etqTidViezB/4ukOAALy23HeMBsJJ8g+2nFaIdbC7E9PhLw0y2YIA1ItjutpqS1/8Ga8fw40mR+ASt2T+/sw==\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a8cfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T03:56:36.402967Z",
     "iopub.status.busy": "2025-07-09T03:56:36.402845Z",
     "iopub.status.idle": "2025-07-09T03:56:36.405220Z",
     "shell.execute_reply": "2025-07-09T03:56:36.404884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define storage account\n",
    "storage_account_land = \"kho\"\n",
    "storage_account_bronze = \"mybronze\"\n",
    "landing_container = \"online-store\"\n",
    "bronze_container = \"bronze-stream\"\n",
    "\n",
    "tables = [\n",
    "    \"Sellers\", \"Customers\", \"ProductCategories\", \"Products\", \"OrderStatus\",\n",
    "    \"Orders\", \"Reasons\", \"OrderItems\", \"ShoppingCarts\", \"CartItems\",\n",
    "    \"PaymentMethods\", \"Payments\", \"Reviews\", \"Addresses\", \"Inventory\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675bb78e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T03:56:36.406368Z",
     "iopub.status.busy": "2025-07-09T03:56:36.406259Z",
     "iopub.status.idle": "2025-07-09T04:02:53.750873Z",
     "shell.execute_reply": "2025-07-09T04:02:53.750478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Sellers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/09 12:30:56 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-azure-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Sellers as parquet\n",
      "Processing Customers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Customers as parquet\n",
      "Processing ProductCategories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ProductCategories as parquet\n",
      "Processing Products...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Products as parquet\n",
      "Processing OrderStatus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved OrderStatus as parquet\n",
      "Processing Orders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Orders as parquet\n",
      "Processing Reasons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Reasons as parquet\n",
      "Processing OrderItems...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved OrderItems as parquet\n",
      "Processing ShoppingCarts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ShoppingCarts as parquet\n",
      "Processing CartItems...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CartItems as parquet\n",
      "Processing PaymentMethods...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PaymentMethods as parquet\n",
      "Processing Payments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Payments as parquet\n",
      "Processing Reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Reviews as parquet\n",
      "Processing Addresses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Addresses as parquet\n",
      "Processing Inventory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Inventory as parquet\n"
     ]
    }
   ],
   "source": [
    "for table in tables:\n",
    "    csv_path = f\"wasbs://{landing_container}@{storage_account_land}.blob.core.windows.net/{table}\"\n",
    "    parquet_path = f\"abfss://{bronze_container}@{storage_account_bronze}.dfs.core.windows.net/{table}\"\n",
    "\n",
    "    print(f\"Processing {table}...\")\n",
    "\n",
    "    df = spark.read.option(\"header\", \"true\").csv(csv_path) # read csv\n",
    "    df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "    print(f\"Saved {table} as parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
