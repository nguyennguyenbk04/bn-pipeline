{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4704ac55",
   "metadata": {},
   "source": [
    "# Data Warehouse Delta Loader\n",
    "\n",
    "This notebook is responsible for loading data into the **Delta Lake** tables in the data warehouse layer.\n",
    "\n",
    "**Important**: Ensure that the Delta Lake tables are properly configured for downstream analytics and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements-delta.txt\n",
    "# python -m ipykernel install --user --name=python3\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, dayofmonth, month, quarter, year, dayofweek, date_format, sum as spark_sum\n",
    "from delta import *\n",
    "\n",
    "jar_dir = \"/path/to/your/jars\"\n",
    "\n",
    "# PYSPARK configuration for Azure Blob Storage integration with Delta Lake\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = (\n",
    "    f\"--jars {jar_dir}/hadoop-azure-3.3.6.jar,\"\n",
    "    f\"{jar_dir}/azure-storage-8.6.6.jar,\"\n",
    "    f\"{jar_dir}/jetty-client-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-http-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-io-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/mysql-connector-j-9.3.0.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-ajax-9.4.43.v20210629.jar \"\n",
    "    \"--packages io.delta:delta-spark_2.12:3.0.0 \"\n",
    "    \"pyspark-shell\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init spark session with Delta Lake support\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"DW Data Load - Delta Lake\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")  # Allow schema evolution\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\")  # Disable vectorized reader to avoid type conflicts\n",
    "    .getOrCreate())\n",
    "\n",
    "print(\"Spark session with Delta Lake support initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL connection for data warehouse\n",
    "mysql_url = \"jdbc:mysql://YOUR_MYSQL_HOST:3306/store_dw\"\n",
    "mysql_props = {\n",
    "    \"user\": \"YOUR_MYSQL_USERNAME\",\n",
    "    \"password\": \"YOUR_MYSQL_PASSWORD\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "print(\"MySQL connection properties configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0483ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure storage access keys\n",
    "\n",
    "# Silver access key (source: silver-delta)\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.YOUR_SILVER_STORAGE_ACCOUNT.dfs.core.windows.net\",\n",
    "    \"YOUR_AZURE_SILVER_STORAGE_KEY\"\n",
    ")\n",
    "\n",
    "# Gold access key (target: gold-delta)\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.YOUR_GOLD_STORAGE_ACCOUNT.dfs.core.windows.net\",\n",
    "    \"YOUR_AZURE_GOLD_STORAGE_KEY\"\n",
    ")\n",
    "\n",
    "print(\"Azure storage credentials configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define storage paths for Delta Lake\n",
    "silver_delta_base = \"abfss://silver-delta@YOUR_SILVER_STORAGE_ACCOUNT.dfs.core.windows.net\"\n",
    "gold_delta_base = \"abfss://gold-delta@YOUR_GOLD_STORAGE_ACCOUNT.dfs.core.windows.net\"\n",
    "\n",
    "print(f\"Source (Silver Delta): {silver_delta_base}\")\n",
    "print(f\"Target (Gold Delta): {gold_delta_base}\")\n",
    "print(\"Building Data Warehouse with Delta Lake format...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5522f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DimProduct - Delta Lake Version\n",
    "print(\"Processing DimProduct...\")\n",
    "\n",
    "products = spark.read.format(\"delta\").load(f\"{silver_delta_base}/Products\")\n",
    "dim_product = products.select(\n",
    "    col(\"ProductID\"),\n",
    "    col(\"Name\").alias(\"ProductName\"),\n",
    "    col(\"CategoryID\"),\n",
    "    col(\"SellerID\")\n",
    ")\n",
    "\n",
    "# Write to MySQL\n",
    "dim_product.write.jdbc(mysql_url, \"DimProduct\", mode=\"overwrite\", properties=mysql_props)\n",
    "\n",
    "# Write to Delta Lake (gold-delta)\n",
    "dim_product.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_delta_base}/DimProduct\")\n",
    "\n",
    "record_count = dim_product.count()\n",
    "print(f\"DimProduct: {record_count} records written to MySQL and Delta Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DimCategory - Delta Lake Version\n",
    "print(\"Processing DimCategory...\")\n",
    "\n",
    "categories = spark.read.format(\"delta\").load(f\"{silver_delta_base}/ProductCategories\")\n",
    "dim_category = categories.select(\n",
    "    col(\"CategoryID\"),\n",
    "    col(\"CategoryName\")\n",
    ")\n",
    "\n",
    "# Write to MySQL\n",
    "dim_category.write.jdbc(mysql_url, \"DimCategory\", mode=\"overwrite\", properties=mysql_props)\n",
    "\n",
    "# Write to Delta Lake (gold-delta)\n",
    "dim_category.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_delta_base}/DimCategory\")\n",
    "\n",
    "record_count = dim_category.count()\n",
    "print(f\"DimCategory: {record_count} records written to MySQL and Delta Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4def40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DimSeller - Delta Lake Version\n",
    "print(\"üìã Processing DimSeller...\")\n",
    "\n",
    "sellers = spark.read.format(\"delta\").load(f\"{silver_delta_base}/Sellers\")\n",
    "dim_seller = sellers.select(\n",
    "    col(\"SellerID\"),\n",
    "    col(\"Name\").alias(\"SellerName\")\n",
    ")\n",
    "\n",
    "# Write to MySQL\n",
    "dim_seller.write.jdbc(mysql_url, \"DimSeller\", mode=\"overwrite\", properties=mysql_props)\n",
    "\n",
    "# Write to Delta Lake (gold-delta)\n",
    "dim_seller.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_delta_base}/DimSeller\")\n",
    "\n",
    "record_count = dim_seller.count()\n",
    "print(f\"DimSeller: {record_count} records written to MySQL and Delta Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76153b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DimCustomer - Delta Lake Version\n",
    "print(\"Processing DimCustomer...\")\n",
    "\n",
    "customers = spark.read.format(\"delta\").load(f\"{silver_delta_base}/Customers\")\n",
    "dim_customer = customers.select(\n",
    "    col(\"CustomerID\"),\n",
    "    col(\"Name\").alias(\"CustomerName\")\n",
    ")\n",
    "\n",
    "# Write to MySQL\n",
    "dim_customer.write.jdbc(mysql_url, \"DimCustomer\", mode=\"overwrite\", properties=mysql_props)\n",
    "\n",
    "# Write to Delta Lake (gold-delta)\n",
    "dim_customer.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_delta_base}/DimCustomer\")\n",
    "\n",
    "record_count = dim_customer.count()\n",
    "print(f\"DimCustomer: {record_count} records written to MySQL and Delta Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. DimOrderStatus - Delta Lake Version\n",
    "print(\"Processing DimOrderStatus...\")\n",
    "\n",
    "order_status = spark.read.format(\"delta\").load(f\"{silver_delta_base}/OrderStatus\")\n",
    "dim_order_status = order_status.select(\n",
    "    col(\"StatusID\"),\n",
    "    col(\"StatusName\")\n",
    ")\n",
    "\n",
    "# Write to MySQL\n",
    "dim_order_status.write.jdbc(mysql_url, \"DimOrderStatus\", mode=\"overwrite\", properties=mysql_props)\n",
    "\n",
    "# Write to Delta Lake (gold-delta)\n",
    "dim_order_status.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_delta_base}/DimOrderStatus\")\n",
    "\n",
    "record_count = dim_order_status.count()\n",
    "print(f\"DimOrderStatus: {record_count} records written to MySQL and Delta Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc95d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. DimDate - Delta Lake Version (from Orders)\n",
    "print(\"Processing DimDate...\")\n",
    "\n",
    "orders = spark.read.format(\"delta\").load(f\"{silver_delta_base}/Orders\")\n",
    "dim_date = orders.select(\n",
    "    date_format(col(\"CreatedAt\"), \"yyyyMMdd\").cast(\"int\").alias(\"DateKey\"),\n",
    "    col(\"CreatedAt\").cast(\"date\").alias(\"Date\"),\n",
    "    dayofmonth(col(\"CreatedAt\")).alias(\"Day\"),\n",
    "    month(col(\"CreatedAt\")).alias(\"Month\"),\n",
    "    quarter(col(\"CreatedAt\")).alias(\"Quarter\"),\n",
    "    year(col(\"CreatedAt\")).alias(\"Year\"),\n",
    "    dayofweek(col(\"CreatedAt\")).alias(\"DayOfWeek\")\n",
    ").distinct()\n",
    "\n",
    "# Write to MySQL\n",
    "dim_date.write.jdbc(mysql_url, \"DimDate\", mode=\"overwrite\", properties=mysql_props)\n",
    "\n",
    "# Write to Delta Lake (gold-delta)\n",
    "dim_date.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_delta_base}/DimDate\")\n",
    "\n",
    "record_count = dim_date.count()\n",
    "print(f\"DimDate: {record_count} records written to MySQL and Delta Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. DimReason - Delta Lake Version\n",
    "print(\"Processing DimReason...\")\n",
    "\n",
    "reasons = spark.read.format(\"delta\").load(f\"{silver_delta_base}/Reasons\")\n",
    "dim_reason = reasons.select(\n",
    "    col(\"ReasonID\"),\n",
    "    col(\"ReasonType\"),\n",
    "    col(\"ReasonDescription\")\n",
    ")\n",
    "\n",
    "# Write to MySQL\n",
    "dim_reason.write.jdbc(mysql_url, \"DimReason\", mode=\"overwrite\", properties=mysql_props)\n",
    "\n",
    "# Write to Delta Lake (gold-delta)\n",
    "dim_reason.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_delta_base}/DimReason\")\n",
    "\n",
    "record_count = dim_reason.count()\n",
    "print(f\"  ‚úÖ DimReason: {record_count} records written to MySQL and Delta Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3e55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. FactSales - Delta Lake Version\n",
    "print(\"üìã Processing FactSales...\")\n",
    "\n",
    "# Read all required tables from Delta Lake\n",
    "order_items = spark.read.format(\"delta\").load(f\"{silver_delta_base}/OrderItems\").alias(\"oi\")\n",
    "orders = spark.read.format(\"delta\").load(f\"{silver_delta_base}/Orders\").alias(\"o\")\n",
    "payments = spark.read.format(\"delta\").load(f\"{silver_delta_base}/Payments\").alias(\"p\")\n",
    "products = spark.read.format(\"delta\").load(f\"{silver_delta_base}/Products\").alias(\"pr\")\n",
    "\n",
    "# Only consider orders that have a payment\n",
    "paid_orders = payments.select(\"OrderID\").distinct().alias(\"po\")\n",
    "\n",
    "# Join OrderItems with Orders and filter for paid orders\n",
    "fact_sales = (\n",
    "    order_items\n",
    "    .join(orders, col(\"oi.OrderID\") == col(\"o.OrderID\"))\n",
    "    .join(paid_orders, col(\"oi.OrderID\") == col(\"po.OrderID\"), \"inner\")\n",
    "    .join(products, col(\"oi.ProductID\") == col(\"pr.ProductID\"))\n",
    "    .join(payments, col(\"oi.OrderID\") == col(\"p.OrderID\"), \"inner\")\n",
    "    .select(\n",
    "        col(\"oi.OrderItemID\"),\n",
    "        col(\"oi.OrderID\"),\n",
    "        col(\"oi.ProductID\"),\n",
    "        col(\"pr.SellerID\"),\n",
    "        col(\"o.CustomerID\"),\n",
    "        col(\"pr.CategoryID\"),\n",
    "        date_format(col(\"o.CreatedAt\"), \"yyyyMMdd\").cast(\"int\").alias(\"OrderDateKey\"),\n",
    "        col(\"o.StatusID\"),\n",
    "        col(\"oi.Quantity\").cast(\"int\").alias(\"Quantity\"),\n",
    "        col(\"pr.Price\").cast(\"double\").alias(\"CurrentPrice\"),  # Use Price from Product\n",
    "        col(\"pr.Cost\").cast(\"double\").alias(\"Cost\"),           # Use Cost from Product\n",
    "        (col(\"oi.Quantity\").cast(\"int\") * col(\"pr.Price\").cast(\"double\")).alias(\"Revenue\"),\n",
    "        (\n",
    "            (col(\"oi.Quantity\").cast(\"int\") * col(\"pr.Price\").cast(\"double\")) -\n",
    "            (col(\"oi.Quantity\").cast(\"int\") * col(\"pr.Cost\").cast(\"double\"))\n",
    "        ).alias(\"Profit\"),\n",
    "        col(\"p.CreatedAt\").alias(\"CreatedAt\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Write to MySQL\n",
    "fact_sales.write.jdbc(mysql_url, \"FactSales\", mode=\"overwrite\", properties=mysql_props)\n",
    "\n",
    "# Write to Delta Lake (gold-delta)\n",
    "fact_sales.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_delta_base}/FactSales\")\n",
    "\n",
    "record_count = fact_sales.count()\n",
    "print(f\"  ‚úÖ FactSales: {record_count} records written to MySQL and Delta Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. FactOrderReason - Delta Lake Version\n",
    "print(\"üìã Processing FactOrderReason...\")\n",
    "\n",
    "# Read required tables from Delta Lake\n",
    "reasons = spark.read.format(\"delta\").load(f\"{silver_delta_base}/Reasons\")\n",
    "order_items = spark.read.format(\"delta\").load(f\"{silver_delta_base}/OrderItems\")\n",
    "products = spark.read.format(\"delta\").load(f\"{silver_delta_base}/Products\")\n",
    "orders = spark.read.format(\"delta\").load(f\"{silver_delta_base}/Orders\")\n",
    "\n",
    "# Filter orders with status 4 or 5 (cancelled/returned orders)\n",
    "filtered_orders = orders.filter(orders.StatusID.isin(4, 5))\n",
    "\n",
    "# Join Reasons with filtered Orders and OrderItems\n",
    "fact_order_reason = (\n",
    "    reasons\n",
    "    .join(filtered_orders, reasons.OrderID == filtered_orders.OrderID, \"inner\")\n",
    "    .join(order_items, reasons.OrderID == order_items.OrderID, \"inner\")\n",
    "    .join(products, order_items.ProductID == products.ProductID, \"inner\")\n",
    "    .select(\n",
    "        order_items.OrderItemID,\n",
    "        reasons.ReasonID,\n",
    "        reasons.OrderID,\n",
    "        products.SellerID,\n",
    "        date_format(filtered_orders.CreatedAt, \"yyyyMMdd\").cast(\"int\").alias(\"OrderDateKey\"),\n",
    "        filtered_orders.StatusID\n",
    "    )\n",
    ")\n",
    "\n",
    "# Write to MySQL\n",
    "fact_order_reason.write.jdbc(mysql_url, \"FactOrderReason\", mode=\"overwrite\", properties=mysql_props)\n",
    "\n",
    "# Write to Delta Lake (gold-delta)\n",
    "fact_order_reason.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_delta_base}/FactOrderReason\")\n",
    "\n",
    "record_count = fact_order_reason.count()\n",
    "print(f\"  ‚úÖ FactOrderReason: {record_count} records written to MySQL and Delta Lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Check all Delta tables in gold-delta\n",
    "print(\"\\nüîç Verifying Gold Delta Lake tables...\\n\")\n",
    "\n",
    "gold_tables = [\n",
    "    \"DimProduct\", \"DimCategory\", \"DimSeller\", \"DimCustomer\", \n",
    "    \"DimOrderStatus\", \"DimDate\", \"DimReason\", \n",
    "    \"FactSales\", \"FactOrderReason\"\n",
    "]\n",
    "\n",
    "total_records = 0\n",
    "for table in gold_tables:\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(f\"{gold_delta_base}/{table}\")\n",
    "        count = df.count()\n",
    "        total_records += count\n",
    "        print(f\"  ‚úÖ {table}: {count} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {table}: Error - {str(e)[:50]}...\")\n",
    "\n",
    "print(f\"\\nüìä Total records in Gold Delta Lake: {total_records:,}\")\n",
    "print(\"\\nüéâ Data Warehouse load completed!\")\n",
    "print(\"‚ú® All data available in both MySQL and Delta Lake (gold-delta)\")\n",
    "print(\"üîÑ Delta Lake provides ACID transactions, time travel, and schema evolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4416d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Show Delta Lake table properties and history\n",
    "print(\"üåü Delta Lake Features for Data Warehouse:\")\n",
    "print(\"\\nüìö Time Travel Examples:\")\n",
    "print(\"   - spark.read.format('delta').option('versionAsOf', 0).load(path)\")\n",
    "print(\"   - spark.read.format('delta').option('timestampAsOf', '2025-01-01').load(path)\")\n",
    "\n",
    "print(\"\\nüîÑ ACID Transactions:\")\n",
    "print(\"   - All dimension and fact table loads are atomic\")\n",
    "print(\"   - Safe concurrent access for BI tools and analytics\")\n",
    "\n",
    "print(\"\\n‚ö° Performance Optimizations:\")\n",
    "print(\"   - OPTIMIZE command for file compaction\")\n",
    "print(\"   - Z-ORDER for multi-dimensional clustering\")\n",
    "print(\"   - Data skipping with statistics\")\n",
    "\n",
    "print(\"\\nüîç Management Commands:\")\n",
    "print(\"   - DESCRIBE HISTORY delta.`/path/to/table`\")\n",
    "print(\"   - OPTIMIZE delta.`/path/to/table` ZORDER BY (column)\")\n",
    "print(\"   - VACUUM delta.`/path/to/table` RETAIN 168 HOURS\")\n",
    "\n",
    "# Show sample table history\n",
    "try:\n",
    "    print(\"\\nüìã Sample: DimProduct table history\")\n",
    "    history_df = spark.sql(f\"DESCRIBE HISTORY delta.`{gold_delta_base}/DimProduct`\")\n",
    "    history_df.select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(5, truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"   Note: History not available yet: {e}\")\n",
    "\n",
    "# Stop Spark session\n",
    "print(\"\\nüõë Stopping Spark session...\")\n",
    "spark.stop()\n",
    "print(\"‚úÖ Data warehouse processing completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
