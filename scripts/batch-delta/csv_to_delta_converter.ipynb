{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acfa50e7",
   "metadata": {},
   "source": [
    "# CSV to Delta Lake Converter\n",
    "\n",
    "This notebook converts CSV files from the landing zone to **Delta Lake format** in the bronze layer.\n",
    "\n",
    "**Important**: This creates Delta Lake tables that are compatible with your streaming CDC pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define the path to the jar files\n",
    "jar_dir = \"/path/to/your/jars\"\n",
    "\n",
    "# PYSPARK configuration for Azure Blob Storage integration with Delta Lake\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = (\n",
    "    f\"--jars {jar_dir}/hadoop-azure-3.3.6.jar,\"\n",
    "    f\"{jar_dir}/azure-storage-8.6.6.jar,\"\n",
    "    f\"{jar_dir}/jetty-client-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-http-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-io-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-ajax-9.4.43.v20210629.jar \"\n",
    "    \"pyspark-shell\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init spark session with Delta Lake support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to Delta Lake Converter\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session with Delta Lake support created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landing zone access key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.YOUR_LANDING_ZONE.blob.core.windows.net\",\n",
    "    \"access_key_for_landing_zone\"  # Replace with your actual access key\n",
    ")\n",
    "\n",
    "# Bronze access key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.YOUR_BRONZE.blob.core.windows.net\",\n",
    "    \"access_key_for_bronze\"  # Replace with your actual access key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c04962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define storage accounts and containers\n",
    "storage_account_land = \"YOUR_LANDING_STORAGE_ACCOUNT\"\n",
    "storage_account_bronze = \"YOUR_BRONZE_STORAGE_ACCOUNT\"\n",
    "landing_container = \"online-store\"\n",
    "bronze_container = \"bronze-delta\"\n",
    "\n",
    "# Tables to convert\n",
    "tables = [\n",
    "    \"Sellers\", \"Customers\", \"ProductCategories\", \"Products\", \"OrderStatus\",\n",
    "    \"Orders\", \"Reasons\", \"OrderItems\", \"ShoppingCarts\", \"CartItems\",\n",
    "    \"Addresses\", \"Inventory\", \"Payments\", \"PaymentMethods\", \"Reviews\"\n",
    "]\n",
    "\n",
    "print(f\"Ready to convert {len(tables)} tables from CSV to Delta Lake format\")\n",
    "print(f\"Source: Landing zone (CSV files)\")\n",
    "print(f\"Target: Bronze layer (Delta Lake format)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CSV files to Delta Lake format\n",
    "success_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for table in tables:\n",
    "    csv_path = f\"wasbs://{landing_container}@{storage_account_land}.blob.core.windows.net/{table}\"\n",
    "    delta_path = f\"abfss://{bronze_container}@{storage_account_bronze}.dfs.core.windows.net/{table}\"\n",
    "\n",
    "    print(f\"Processing {table}...\")\n",
    "\n",
    "    try:\n",
    "        # Read CSV file\n",
    "        df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_path)\n",
    "        record_count = df.count()\n",
    "        \n",
    "        if record_count > 0:\n",
    "            print(f\"   Sample schema for {table}:\")\n",
    "            df.printSchema()\n",
    "            \n",
    "            # Write to Delta Lake\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "            print(f\"   Successfully converted {record_count} records to Delta format\")\n",
    "            success_count += 1\n",
    "        else:\n",
    "            print(f\"   Warning: {table} is empty\")\n",
    "            error_count += 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   Error processing {table}: {e}\")\n",
    "        error_count += 1\n",
    "\n",
    "print(f\"\\nConversion Summary:\")\n",
    "print(f\"Successfully converted: {success_count} tables\")\n",
    "print(f\"Errors encountered: {error_count} tables\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
