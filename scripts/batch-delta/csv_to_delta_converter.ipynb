{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acfa50e7",
   "metadata": {},
   "source": [
    "# CSV to Delta Lake Converter\n",
    "\n",
    "This notebook converts CSV files from the landing zone to **Delta Lake format** in the bronze layer.\n",
    "\n",
    "**Important**: This creates Delta Lake tables that are compatible with your streaming CDC pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "jar_dir = \"/home/bnguyen/Desktop/DE_project/scripts/jars\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    f\"--jars {jar_dir}/hadoop-azure-3.3.6.jar,\"\n",
    "    f\"{jar_dir}/hadoop-common-3.3.6.jar,\"\n",
    "    f\"{jar_dir}/azure-storage-8.6.6.jar,\"\n",
    "    f\"{jar_dir}/jetty-client-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-http-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-io-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-ajax-9.4.43.v20210629.jar \"\n",
    "    \"pyspark-shell\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init spark session with Delta Lake support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to Delta Lake Converter\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark session with Delta Lake support created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landing zone access key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.kho.blob.core.windows.net\",\n",
    "    \"DV9ioy7PXW5gTHcotPx6/ILeKjue7zHgGoNYkt6dvPjqK5XyfK4DVXzfFxmIVGWNj6GVU8Q3a0xl+AStBjZBqg==\"\n",
    ")\n",
    "\n",
    "# Bronze access key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mybronze.dfs.core.windows.net\",\n",
    "    \"c5etqTidViezB/4ukOAALy23HeMBsJJ8g+2nFaIdbC7E9PhLw0y2YIA1ItjutpqS1/8Ga8fw40mR+ASt2T+/sw==\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c04962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define storage accounts and containers\n",
    "storage_account_land = \"kho\"\n",
    "storage_account_bronze = \"mybronze\"\n",
    "landing_container = \"online-store\"\n",
    "bronze_container = \"bronze-delta\"\n",
    "\n",
    "# Tables to convert\n",
    "tables = [\n",
    "    \"Sellers\", \"Customers\", \"ProductCategories\", \"Products\", \"OrderStatus\",\n",
    "    \"Orders\", \"Reasons\", \"OrderItems\", \"ShoppingCarts\", \"CartItems\",\n",
    "    \"PaymentMethods\", \"Payments\", \"Reviews\", \"Addresses\", \"Inventory\"\n",
    "]\n",
    "\n",
    "print(f\"üìã Ready to convert {len(tables)} tables from CSV to Delta Lake format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CSV files to Delta Lake tables\n",
    "success_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for table in tables:\n",
    "    csv_path = f\"wasbs://{landing_container}@{storage_account_land}.blob.core.windows.net/{table}\"\n",
    "    delta_path = f\"abfss://{bronze_container}@{storage_account_bronze}.dfs.core.windows.net/{table}\"\n",
    "\n",
    "    print(f\"üîÑ Processing {table}...\")\n",
    "\n",
    "    try:\n",
    "        # Read CSV file\n",
    "        df = spark.read.option(\"header\", \"true\").csv(csv_path)\n",
    "        record_count = df.count()\n",
    "        \n",
    "        # Show schema for first table as example\n",
    "        if table == tables[0]:\n",
    "            print(f\"   üìã Sample schema for {table}:\")\n",
    "            df.printSchema()\n",
    "        \n",
    "        # Write as Delta Lake table\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "        \n",
    "        print(f\"   ‚úÖ Saved {table} as Delta Lake table ({record_count:,} records)\")\n",
    "        success_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error processing {table}: {e}\")\n",
    "        error_count += 1\n",
    "\n",
    "print(f\"\\n Conversion completed\")\n",
    "print(f\"   Success: {success_count} tables\")\n",
    "print(f\"    Errors: {error_count} tables\")\n",
    "print(f\"\\n All successful tables are now in Delta Lake format and ready for CDC streaming\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
