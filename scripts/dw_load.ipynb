{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ddce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt\n",
    "# python -m ipykernel install --user --name=python3\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, dayofmonth, month, quarter, year, dayofweek, date_format, sum as spark_sum\n",
    "\n",
    "jar_dir = \"/home/bnguyen/Desktop/DE_project/scripts/jars\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    f\"--jars {jar_dir}/hadoop-azure-3.3.6.jar,\"\n",
    "    f\"{jar_dir}/azure-storage-8.6.6.jar,\"\n",
    "    f\"{jar_dir}/hadoop-common-3.3.6.jar,\"\n",
    "    f\"{jar_dir}/jetty-client-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-http-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-io-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/mysql-connector-j-9.3.0.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-ajax-9.4.43.v20210629.jar \"\n",
    "    \"pyspark-shell\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca086e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/16 10:08:55 WARN Utils: Your hostname, lenovo-slim resolves to a loopback address: 127.0.1.1; using 192.168.199.13 instead (on interface wlp2s0)\n",
      "25/07/16 10:08:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/16 10:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/16 10:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Init spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DW data load\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be60a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_url = \"jdbc:mysql://localhost:3306/store_dw\"\n",
    "mysql_props = {\n",
    "    \"user\": \"bnguyen\",\n",
    "    \"password\": \".Tldccmcbtldck2\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aae5458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/16 10:09:17 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Silver access key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mysilver.blob.core.windows.net\",\n",
    "    \"bAthp0pVBfqEtyCvJElSX7MeI7ejSLa6cjuPoMz0Gg/69uzEW01y4URMDXsdFCrkpc9M54cDHnXs+AStj1gExQ==\"\n",
    ")\n",
    "\n",
    "# Gold\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mygold.dfs.core.windows.net\",\n",
    "    \"wRPXTwWCVxWwUpavEh62A5wzLdUvRTGeB3tZKP3eRbig7ca8ZN51l0kWS32kcbH/ddQ/jNXBzqDC+AStOzXlyw==\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25fc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. DimProduct\n",
    "products = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Products\")\n",
    "dim_product = products.select(\n",
    "    col(\"ProductID\"),\n",
    "    col(\"Name\").alias(\"ProductName\"),\n",
    "    col(\"CategoryID\"),\n",
    "    col(\"SellerID\")\n",
    ")\n",
    "# dim_product.write.jdbc(mysql_url, \"DimProduct\", mode=\"append\", properties=mysql_props)\n",
    "# dim_product.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimProduct\")\n",
    "# dim_product.coalesce(1).write.format(\"csv\").options(header=\"True\", delimiter = ',').mode(\"overwrite\").option(\"path\", \"abfss://gold-csv@mygold.dfs.core.windows.net/DimProduct.csv\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e118e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2. DimCategory\n",
    "categories = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/ProductCategories\")\n",
    "dim_category = categories.select(\n",
    "    col(\"CategoryID\"),\n",
    "    col(\"CategoryName\")\n",
    ")\n",
    "# dim_category.write.jdbc(mysql_url, \"DimCategory\", mode=\"append\", properties=mysql_props)\n",
    "# dim_category.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimCategory\")\n",
    "# dim_category.coalesce(1).write.format(\"csv\").options(header=\"True\", delimiter = ',').mode(\"overwrite\").option(\"path\", \"abfss://gold-csv@mygold.dfs.core.windows.net/DimCategory.csv\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3. DimSeller\n",
    "sellers = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Sellers\")\n",
    "dim_seller = sellers.select(\n",
    "    col(\"SellerID\"),\n",
    "    col(\"Name\").alias(\"SellerName\")\n",
    ")\n",
    "# dim_seller.write.jdbc(mysql_url, \"DimSeller\", mode=\"append\", properties=mysql_props)\n",
    "# dim_seller.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimSeller\")\n",
    "# dim_seller.coalesce(1).write.format(\"csv\").options(header=\"True\", delimiter = ',').mode(\"overwrite\").option(\"path\", \"abfss://gold-csv@mygold.dfs.core.windows.net/DimSeller.csv\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349af045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 4. DimCustomer\n",
    "customers = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Customers\")\n",
    "dim_customer = customers.select(\n",
    "    col(\"CustomerID\"),\n",
    "    col(\"Name\").alias(\"CustomerName\")\n",
    ")\n",
    "# dim_customer.write.jdbc(mysql_url, \"DimCustomer\", mode=\"append\", properties=mysql_props)\n",
    "# dim_customer.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimCustomer\")\n",
    "# dim_customer.coalesce(1).write.format(\"csv\").options(header=\"True\", delimiter = ',').mode(\"overwrite\").option(\"path\", \"abfss://gold-csv@mygold.dfs.core.windows.net/DimCustomer.csv\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be3932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 5. DimOrderStatus\n",
    "order_status = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/OrderStatus\")\n",
    "dim_order_status = order_status.select(\n",
    "    col(\"StatusID\"),\n",
    "    col(\"StatusName\")\n",
    ")\n",
    "# dim_order_status.write.jdbc(mysql_url, \"DimOrderStatus\", mode=\"append\", properties=mysql_props)\n",
    "# dim_order_status.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimOrderStatus\")\n",
    "# dim_order_status.coalesce(1).write.format(\"csv\").options(header=\"True\", delimiter = ',').mode(\"overwrite\").option(\"path\", \"abfss://gold-csv@mygold.dfs.core.windows.net/DimOrderStatus.csv\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5bc0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 6. DimDate (from Orders)\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\")\n",
    "dim_date = orders.select(\n",
    "    date_format(col(\"CreatedAt\"), \"yyyyMMdd\").cast(\"int\").alias(\"DateKey\"),\n",
    "    col(\"CreatedAt\").cast(\"date\").alias(\"Date\"),\n",
    "    dayofmonth(col(\"CreatedAt\")).alias(\"Day\"),\n",
    "    month(col(\"CreatedAt\")).alias(\"Month\"),\n",
    "    quarter(col(\"CreatedAt\")).alias(\"Quarter\"),\n",
    "    year(col(\"CreatedAt\")).alias(\"Year\"),\n",
    "    dayofweek(col(\"CreatedAt\")).alias(\"DayOfWeek\")\n",
    ").distinct()\n",
    "# dim_date.write.jdbc(mysql_url, \"DimDate\", mode=\"append\", properties=mysql_props)\n",
    "# dim_date.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimDate\")\n",
    "# dim_date.coalesce(1).write.format(\"csv\").options(header=\"True\", delimiter = ',').mode(\"overwrite\").option(\"path\", \"abfss://gold-csv@mygold.dfs.core.windows.net/DimDate.csv\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4b9db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 7. DimReason\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m reasons = \u001b[43mspark\u001b[49m.read.parquet(\u001b[33m\"\u001b[39m\u001b[33mwasbs://silver@mysilver.blob.core.windows.net/Reasons\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m dim_reason = reasons.select(\n\u001b[32m      4\u001b[39m     col(\u001b[33m\"\u001b[39m\u001b[33mReasonID\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m     col(\u001b[33m\"\u001b[39m\u001b[33mReasonType\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      6\u001b[39m     col(\u001b[33m\"\u001b[39m\u001b[33mReasonDescription\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# dim_reason.write.jdbc(mysql_url, \"DimReason\", mode=\"append\", properties=mysql_props)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# dim_reason.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimReason\")\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 7. DimReason\n",
    "reasons = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Reasons\")\n",
    "dim_reason = reasons.select(\n",
    "    col(\"ReasonID\"),\n",
    "    col(\"ReasonType\"),\n",
    "    col(\"ReasonDescription\")\n",
    ")\n",
    "# dim_reason.write.jdbc(mysql_url, \"DimReason\", mode=\"append\", properties=mysql_props)\n",
    "# dim_reason.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimReason\")\n",
    "# dim_reason.coalesce(1).write.format(\"csv\").options(header=\"True\", delimiter = ',').mode(\"overwrite\").option(\"path\", \"abfss://gold-csv@mygold.dfs.core.windows.net/DimReason.csv\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e758f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fact sales\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m order_items = \u001b[43mspark\u001b[49m.read.parquet(\u001b[33m\"\u001b[39m\u001b[33mwasbs://silver@mysilver.blob.core.windows.net/OrderItems\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33moi\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m orders = spark.read.parquet(\u001b[33m\"\u001b[39m\u001b[33mwasbs://silver@mysilver.blob.core.windows.net/Orders\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mo\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m payments = spark.read.parquet(\u001b[33m\"\u001b[39m\u001b[33mwasbs://silver@mysilver.blob.core.windows.net/Payments\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mp\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Fact sales\n",
    "order_items = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/OrderItems\").alias(\"oi\")\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\").alias(\"o\")\n",
    "payments = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Payments\").alias(\"p\")\n",
    "products = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Products\").alias(\"pr\")\n",
    "\n",
    "# Only consider orders that have a payment\n",
    "paid_orders = payments.select(\"OrderID\").distinct().alias(\"po\")\n",
    "\n",
    "# Join OrderItems with Orders and filter for paid orders\n",
    "fact_sales = (\n",
    "    order_items\n",
    "    .join(orders, col(\"oi.OrderID\") == col(\"o.OrderID\"))\n",
    "    .join(paid_orders, col(\"oi.OrderID\") == col(\"po.OrderID\"), \"inner\")\n",
    "    .join(products, col(\"oi.ProductID\") == col(\"pr.ProductID\"))\n",
    "    .join(payments, col(\"oi.OrderID\") == col(\"p.OrderID\"), \"inner\")\n",
    "    .select(\n",
    "        col(\"oi.OrderItemID\"),\n",
    "        col(\"oi.OrderID\"),\n",
    "        col(\"oi.ProductID\"),\n",
    "        col(\"pr.SellerID\"),\n",
    "        col(\"o.CustomerID\"),\n",
    "        col(\"pr.CategoryID\"),\n",
    "        date_format(col(\"o.CreatedAt\"), \"yyyyMMdd\").cast(\"int\").alias(\"OrderDateKey\"),\n",
    "        col(\"o.StatusID\"),\n",
    "        col(\"oi.Quantity\").cast(\"int\").alias(\"Quantity\"),\n",
    "        col(\"pr.Price\").cast(\"double\").alias(\"CurrentPrice\"),  # Use Price from Product\n",
    "        col(\"pr.Cost\").cast(\"double\").alias(\"Cost\"),           # Use Cost from Product\n",
    "        (col(\"oi.Quantity\").cast(\"int\") * col(\"pr.Price\").cast(\"double\")).alias(\"Revenue\"),\n",
    "        (\n",
    "            (col(\"oi.Quantity\").cast(\"int\") * col(\"pr.Price\").cast(\"double\")) -\n",
    "            (col(\"oi.Quantity\").cast(\"int\") * col(\"pr.Cost\").cast(\"double\"))\n",
    "        ).alias(\"Profit\"),\n",
    "        col(\"p.CreatedAt\").alias(\"CreatedAt\")\n",
    "    )\n",
    ")\n",
    "# fact_sales.write.jdbc(mysql_url, \"FactSales\", mode=\"overwrite\", properties=mysql_props)\n",
    "# fact_sales.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/FactSales\")\n",
    "# fact_sales.coalesce(1).write.format(\"csv\").options(header=\"True\", delimiter = ',').mode(\"overwrite\").option(\"path\", \"abfss://gold-csv@mygold.dfs.core.windows.net/FactSales.csv\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "803bbfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o260.parquet.\n: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.azurebfs.services.AbfsThrottlingInterceptFactory\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.<init>(AbfsClient.java:130)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.<init>(AbfsClient.java:183)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:1631)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:242)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:197)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.ExceptionInInitializerError: Exception java.lang.NoClassDefFoundError: org/apache/hadoop/util/WeakReferenceMap [in thread \"Thread-4\"]\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsThrottlingInterceptFactory.<clinit>(AbfsThrottlingInterceptFactory.java:52)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      8\u001b[39m fact_order_reason = (\n\u001b[32m      9\u001b[39m     reasons\n\u001b[32m     10\u001b[39m     .join(order_items, reasons.OrderID == order_items.OrderID, \u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     )\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m fact_order_reason.write.jdbc(mysql_url, \u001b[33m\"\u001b[39m\u001b[33mFactOrderReason\u001b[39m\u001b[33m\"\u001b[39m, mode=\u001b[33m\"\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m\"\u001b[39m, properties=mysql_props)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mfact_order_reason\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mabfss://gold-test@mygold.dfs.core.windows.net/FactOrderReason\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# fact_order_reason.coalesce(1).write.format(\"csv\").options(header=\"True\", delimiter = ',').mode(\"overwrite\").option(\"path\", \"abfss://gold-csv@mygold.dfs.core.windows.net/FactOrderReason.csv\").save()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DE_project/venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1721\u001b[39m, in \u001b[36mDataFrameWriter.parquet\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   1719\u001b[39m     \u001b[38;5;28mself\u001b[39m.partitionBy(partitionBy)\n\u001b[32m   1720\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression)\n\u001b[32m-> \u001b[39m\u001b[32m1721\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DE_project/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DE_project/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DE_project/venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o260.parquet.\n: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.azurebfs.services.AbfsThrottlingInterceptFactory\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.<init>(AbfsClient.java:130)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.<init>(AbfsClient.java:183)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:1631)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:242)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:197)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.ExceptionInInitializerError: Exception java.lang.NoClassDefFoundError: org/apache/hadoop/util/WeakReferenceMap [in thread \"Thread-4\"]\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsThrottlingInterceptFactory.<clinit>(AbfsThrottlingInterceptFactory.java:52)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "# FactOrderReason (updated for new schema)\n",
    "reasons = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Reasons\")\n",
    "order_items = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/OrderItems\")\n",
    "products = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Products\")\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\")\n",
    "\n",
    "# Join Reasons with OrderItems to get OrderItemID and SellerID\n",
    "fact_order_reason = (\n",
    "    reasons\n",
    "    .join(order_items, reasons.OrderID == order_items.OrderID, \"inner\")\n",
    "    .join(products, order_items.ProductID == products.ProductID, \"inner\")\n",
    "    .join(orders, reasons.OrderID == orders.OrderID, \"inner\")\n",
    "    .select(\n",
    "        order_items.OrderItemID,\n",
    "        reasons.ReasonID,\n",
    "        reasons.OrderID,\n",
    "        products.SellerID,\n",
    "        date_format(orders.CreatedAt, \"yyyyMMdd\").cast(\"int\").alias(\"OrderDateKey\"),\n",
    "        orders.StatusID\n",
    "    )\n",
    ")\n",
    "fact_order_reason.write.jdbc(mysql_url, \"FactOrderReason\", mode=\"overwrite\", properties=mysql_props)\n",
    "fact_order_reason.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/FactOrderReason\")\n",
    "# fact_order_reason.coalesce(1).write.format(\"csv\").options(header=\"True\", delimiter = ',').mode(\"overwrite\").option(\"path\", \"abfss://gold-csv@mygold.dfs.core.windows.net/FactOrderReason.csv\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_order_reason.write.mode(\"append\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/FactOrderReason\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
