{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c96c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt\n",
    "# python -m ipykernel install --user --name=python3\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, dayofmonth, month, quarter, year, dayofweek, date_format, sum as spark_sum\n",
    "\n",
    "jar_dir = \"/home/bnguyen/Desktop/DE_project/scripts/jars\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    f\"--jars {jar_dir}/hadoop-azure-3.3.6.jar,\"\n",
    "    f\"{jar_dir}/azure-storage-8.6.6.jar,\"\n",
    "    f\"{jar_dir}/hadoop-common-3.3.6.jar,\"\n",
    "    f\"{jar_dir}/jetty-client-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-http-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-io-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/mysql-connector-j-9.3.0.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-ajax-9.4.43.v20210629.jar \"\n",
    "    \"pyspark-shell\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde67285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 09:08:02 WARN Utils: Your hostname, lenovo-slim resolves to a loopback address: 127.0.1.1; using 192.168.199.13 instead (on interface wlp2s0)\n",
      "25/07/21 09:08:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/21 09:08:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/21 09:08:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Init spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test CDC\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be76b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze access key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mybronze.dfs.core.windows.net\",\n",
    "    \"c5etqTidViezB/4ukOAALy23HeMBsJJ8g+2nFaIdbC7E9PhLw0y2YIA1ItjutpqS1/8Ga8fw40mR+ASt2T+/sw==\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e144a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 09:08:29 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Define storage account \n",
    "storage_account_bronze = \"mybronze\"\n",
    "bronze_stream = \"bronze-stream\"\n",
    "bronze_final = \"bronze-final\"\n",
    "bronze_container = \"bronze\"\n",
    "\n",
    "tables = [\"Customers\", \"Products\", \n",
    "          \"Sellers\", \"Orders\", \n",
    "          \"OrderItems\",\"ProductCategories\",\n",
    "          \"OrderStatus\",\"Reasons\", \"Payments\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c76a5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|deleted_id|table_name| status|\n",
      "+----------+----------+-------+\n",
      "|  10000004| Customers|deleted|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bronze_path = f\"abfss://{bronze_final}@{storage_account_bronze}.dfs.core.windows.net/Customers/deleted_10000004.marker\"\n",
    "df = spark.read.parquet(bronze_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e17aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----------------+-----------+--------------------+--------------------+\n",
      "|CustomerID|Name|            Email|PhoneNumber|           CreatedAt|           UpdatedAt|\n",
      "+----------+----+-----------------+-----------+--------------------+--------------------+\n",
      "|  10000004| ABC|alice@example.com| 0123456789|2025-07-21T02:22:32Z|2025-07-21T02:25:03Z|\n",
      "+----------+----+-----------------+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# INSERT INTO Customers (CustomerID, Name, Email, PhoneNumber) VALUES (10000004, 'Alice Ng', 'alice@example.com', '0123456789');\n",
    "# DELETE FROM Customers WHERE CustomerID = 10000004;\n",
    "\n",
    "\n",
    "\n",
    "# Show record with CustomerID = 10000004 from bronze-final\n",
    "bronze_path = f\"abfss://{bronze_final}@{storage_account_bronze}.dfs.core.windows.net/Customers/\"\n",
    "df = spark.read.parquet(bronze_path)\n",
    "df.filter(df.CustomerID == 10000004).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8be7433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Customers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Copied 7000 records\n",
      "Processing Products...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Copied 6000 records\n",
      "Processing Sellers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Copied 4000 records\n",
      "Processing Orders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Copied 100000 records\n",
      "Processing OrderItems...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Copied 300756 records\n",
      "Processing ProductCategories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Copied 10 records\n",
      "Processing OrderStatus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Copied 5 records\n",
      "Processing Reasons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Copied 4931 records\n",
      "Processing Payments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Copied 80200 records\n",
      "\n",
      "Initial merge completed!\n"
     ]
    }
   ],
   "source": [
    "# Initial merge: Copy bronze data to bronze-final and silver-final\n",
    "# This creates the baseline for the continuous merge process\n",
    "\n",
    "# Configure bronze-final storage\n",
    "storage_account_bronze_final = \"mybronze\"\n",
    "storage_account_silver_final = \"mysilver\"\n",
    "bronze_final_container = \"bronze-final\"\n",
    "silver_final_container = \"silver-final\"\n",
    "\n",
    "# Set access key for bronze-final (same as bronze)\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mybronze.dfs.core.windows.net\",\n",
    "    \"c5etqTidViezB/4ukOAALy23HeMBsJJ8g+2nFaIdbC7E9PhLw0y2YIA1ItjutpqS1/8Ga8fw40mR+ASt2T+/sw==\"\n",
    ")\n",
    "\n",
    "spark.conf.set(\n",
    "        \"fs.azure.account.key.mysilver.dfs.core.windows.net\",\n",
    "        \"bAthp0pVBfqEtyCvJElSX7MeI7ejSLa6cjuPoMz0Gg/69uzEW01y4URMDXsdFCrkpc9M54cDHnXs+AStj1gExQ==\"\n",
    "    )\n",
    "\n",
    "for table in tables:\n",
    "    bronze_path = f\"abfss://{bronze_container}@{storage_account_bronze}.dfs.core.windows.net/{table}\"\n",
    "    bronze_final_path = f\"abfss://{bronze_final_container}@{storage_account_bronze_final}.dfs.core.windows.net/{table}\"\n",
    "    silver_final_path = f\"abfss://{silver_final_container}@{storage_account_silver_final}.dfs.core.windows.net/{table}\"\n",
    "\n",
    "    print(f\"Processing {table}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read from bronze\n",
    "        df = spark.read.parquet(bronze_path)\n",
    "        record_count = df.count()\n",
    "        \n",
    "        # Write with overwrite mode\n",
    "        df.write.mode(\"overwrite\").parquet(bronze_final_path)\n",
    "        df.write.mode(\"overwrite\").parquet(silver_final_path)\n",
    "        \n",
    "        print(f\"  ✓ Copied {record_count} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing {table}: {e}\")\n",
    "\n",
    "print(\"\\nInitial merge completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2353532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Customers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📊 Found 7000 records in bronze\n",
      "  📋 Sample data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------------+------------------+-------------------+-------------------+\n",
      "|CustomerID|           Name|               Email|       PhoneNumber|          CreatedAt|          UpdatedAt|\n",
      "+----------+---------------+--------------------+------------------+-------------------+-------------------+\n",
      "|         1|   Charles Park| tmiller@example.com|793-701-5921x92794|2024-02-26 17:56:18|2024-11-20 20:17:14|\n",
      "|         2|Michael Estrada|  rbrown@example.org|        6953312991|2023-12-09 16:51:08|2025-06-28 11:45:07|\n",
      "|         3| Gail Wilkerson|graybrittany@exam...|      550.265.9882|2024-02-09 17:10:59|2025-05-26 07:26:49|\n",
      "|         4|Alexandra Moyer|  mark96@example.org|   +1-978-296-4775|2024-11-26 14:08:20|2024-09-27 03:46:33|\n",
      "|         5|  Natasha Perry| katie74@example.com|      457.834.5540|2024-11-17 05:40:49|2024-07-17 05:36:49|\n",
      "+----------+---------------+--------------------+------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Successfully copied 7000 records to bronze-final\n",
      "  📁 Destination: abfss://bronze-final@mybronze.dfs.core.windows.net/Customers\n",
      "\n",
      "Customers table is ready for continuous CDC merging!\n"
     ]
    }
   ],
   "source": [
    "# Copy content for just the Customers table\n",
    "# This will create the initial baseline for the Customers table in bronze-final\n",
    "\n",
    "# Configure paths\n",
    "table = \"Customers\"\n",
    "bronze_path = f\"abfss://{bronze_container}@{storage_account_bronze}.dfs.core.windows.net/{table}\"\n",
    "bronze_final_path = f\"abfss://bronze-final@{storage_account_bronze}.dfs.core.windows.net/{table}\"\n",
    "silver_final_path = f\"abfss://{silver_final_container}@{storage_account_silver_final}.dfs.core.windows.net/{table}\"\n",
    "\n",
    "print(f\"Processing {table}...\")\n",
    "\n",
    "try:\n",
    "    # Read from bronze\n",
    "    df = spark.read.parquet(bronze_path)\n",
    "    record_count = df.count()\n",
    "    \n",
    "    print(f\"  📊 Found {record_count} records in bronze\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"  📋 Sample data:\")\n",
    "    df.show(5)\n",
    "    \n",
    "    # Write to bronze-final with overwrite mode\n",
    "    df.write.mode(\"overwrite\").parquet(bronze_final_path)\n",
    "    df.write.mode(\"overwrite\").parquet(silver_final_path)\n",
    "    \n",
    "    print(f\"  ✓ Successfully copied {record_count} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error processing {table}: {e}\")\n",
    "\n",
    "print(f\"\\n{table} table is ready for continuous CDC merging!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
