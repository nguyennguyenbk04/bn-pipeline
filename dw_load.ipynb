{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ddce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, dayofmonth, month, quarter, year, dayofweek, date_format, sum as spark_sum\n",
    "\n",
    "# Set environment vars to load jars\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--jars jars/hadoop-azure-3.3.6.jar,\" # Hadoop connector (wasb + wasbs)\n",
    "    \"jars/mysql-connector-j-9.3.0.jar,\"\n",
    "    \"jars/azure-storage-8.6.6.jar,\" # Azure SDK for Java (allow communication between Hadoop, Spark with Blobs Storage)\n",
    "    \"jars/jetty-client-9.4.43.v20210629.jar,\" # I don't know...\n",
    "    \"jars/jetty-http-9.4.43.v20210629.jar,\" ###############\n",
    "    \"jars/jetty-io-9.4.43.v20210629.jar,\" #################\n",
    "    \"jars/jetty-util-9.4.43.v20210629.jar,\" ################\n",
    "    \"jars/jetty-util-ajax-9.4.43.v20210629.jar \" ############\n",
    "    \"pyspark-shell\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca086e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/07 14:05:43 WARN Utils: Your hostname, lenovo-slim, resolves to a loopback address: 127.0.1.1; using 192.168.199.13 instead (on interface wlp2s0)\n",
      "25/07/07 14:05:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/07 14:05:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/07 14:05:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Init spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DW data load\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be60a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_url = \"jdbc:mysql://localhost:3306/store_dw\"\n",
    "mysql_props = {\n",
    "    \"user\": \"bnguyen\",\n",
    "    \"password\": \".Tldccmcbtldck2\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aae5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver access key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mysilver.blob.core.windows.net\",\n",
    "    \"bAthp0pVBfqEtyCvJElSX7MeI7ejSLa6cjuPoMz0Gg/69uzEW01y4URMDXsdFCrkpc9M54cDHnXs+AStj1gExQ==\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25fc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. DimProduct\n",
    "products = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Products\")\n",
    "dim_product = products.select(\n",
    "    col(\"ProductID\"),\n",
    "    col(\"Name\").alias(\"ProductName\"),\n",
    "    col(\"CategoryID\"),\n",
    "    col(\"SellerID\")\n",
    ")\n",
    "dim_product.write.jdbc(mysql_url, \"DimProduct\", mode=\"append\", properties=mysql_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e118e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2. DimCategory\n",
    "categories = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/ProductCategories\")\n",
    "dim_category = categories.select(\n",
    "    col(\"CategoryID\"),\n",
    "    col(\"CategoryName\")\n",
    ")\n",
    "dim_category.write.jdbc(mysql_url, \"DimCategory\", mode=\"append\", properties=mysql_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7551ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3. DimSeller\n",
    "sellers = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Sellers\")\n",
    "dim_seller = sellers.select(\n",
    "    col(\"SellerID\"),\n",
    "    col(\"Name\").alias(\"SellerName\")\n",
    ")\n",
    "dim_seller.write.jdbc(mysql_url, \"DimSeller\", mode=\"append\", properties=mysql_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "349af045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 4. DimCustomer\n",
    "customers = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Customers\")\n",
    "dim_customer = customers.select(\n",
    "    col(\"CustomerID\"),\n",
    "    col(\"Name\").alias(\"CustomerName\")\n",
    ")\n",
    "dim_customer.write.jdbc(mysql_url, \"DimCustomer\", mode=\"append\", properties=mysql_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77be3932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 5. DimOrderStatus\n",
    "order_status = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/OrderStatus\")\n",
    "dim_order_status = order_status.select(\n",
    "    col(\"StatusID\"),\n",
    "    col(\"StatusName\")\n",
    ")\n",
    "dim_order_status.write.jdbc(mysql_url, \"DimOrderStatus\", mode=\"append\", properties=mysql_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e5bc0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 6. DimDate (from Orders)\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\")\n",
    "dim_date = orders.select(\n",
    "    date_format(col(\"CreatedAt\"), \"yyyyMMdd\").cast(\"int\").alias(\"DateKey\"),\n",
    "    col(\"CreatedAt\").cast(\"date\").alias(\"Date\"),\n",
    "    dayofmonth(col(\"CreatedAt\")).alias(\"Day\"),\n",
    "    month(col(\"CreatedAt\")).alias(\"Month\"),\n",
    "    quarter(col(\"CreatedAt\")).alias(\"Quarter\"),\n",
    "    year(col(\"CreatedAt\")).alias(\"Year\"),\n",
    "    dayofweek(col(\"CreatedAt\")).alias(\"DayOfWeek\")\n",
    ").distinct()\n",
    "dim_date.write.jdbc(mysql_url, \"DimDate\", mode=\"append\", properties=mysql_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f4b9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 7. DimReason\n",
    "reasons = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Reasons\")\n",
    "dim_reason = reasons.select(\n",
    "    col(\"ReasonID\"),\n",
    "    col(\"ReasonType\"),\n",
    "    col(\"ReasonDescription\")\n",
    ")\n",
    "dim_reason.write.jdbc(mysql_url, \"DimReason\", mode=\"append\", properties=mysql_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 14:06:08 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-azure-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fact sales\n",
    "order_items = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/OrderItems\")\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\")\n",
    "payments = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Payments\")\n",
    "products = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Products\")\n",
    "\n",
    "# Only consider orders that have a payment\n",
    "paid_orders = payments.select(\"OrderID\").distinct()\n",
    "\n",
    "# Join OrderItems with Orders and filter for paid orders\n",
    "fact_sales = (\n",
    "    order_items\n",
    "    .join(orders, order_items.OrderID == orders.OrderID)\n",
    "    .join(paid_orders, order_items.OrderID == paid_orders.OrderID, \"inner\")\n",
    "    .join(products, order_items.ProductID == products.ProductID)\n",
    "    .select(\n",
    "        order_items.OrderItemID,\n",
    "        order_items.OrderID,\n",
    "        order_items.ProductID,\n",
    "        products.SellerID,\n",
    "        orders.CustomerID,\n",
    "        products.CategoryID,\n",
    "        date_format(orders.CreatedAt, \"yyyyMMdd\").cast(\"int\").alias(\"OrderDateKey\"),\n",
    "        orders.StatusID,\n",
    "        col(\"Quantity\").cast(\"int\").alias(\"Quantity\"),\n",
    "        col(\"CurrentPrice\").cast(\"double\").alias(\"CurrentPrice\"),\n",
    "        # Revenue only for paid orders, cast both to numeric\n",
    "        (col(\"Quantity\").cast(\"int\") * col(\"CurrentPrice\").cast(\"double\")).alias(\"Revenue\")\n",
    "    )\n",
    ")\n",
    "fact_sales.write.jdbc(mysql_url, \"FactSales\", mode=\"append\", properties=mysql_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803bbfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fact reasons\n",
    "reasons = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Reasons\")\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\")\n",
    "\n",
    "# Join Reasons with Orders to get OrderDateKey and StatusID\n",
    "fact_order_reason = (\n",
    "    reasons\n",
    "    .join(orders, reasons.OrderID == orders.OrderID, \"inner\")\n",
    "    .select(\n",
    "        reasons.ReasonID,\n",
    "        reasons.OrderID,\n",
    "        date_format(orders.CreatedAt, \"yyyyMMdd\").cast(\"int\").alias(\"OrderDateKey\"),\n",
    "        orders.StatusID\n",
    "    )\n",
    ")\n",
    "fact_order_reason.write.jdbc(mysql_url, \"FactOrderReason\", mode=\"append\", properties=mysql_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b72cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fact customer order\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\")\n",
    "order_items = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/OrderItems\")\n",
    "payments = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Payments\")\n",
    "\n",
    "# Only include paid orders\n",
    "paid_orders = payments.select(\"OrderID\").distinct()\n",
    "\n",
    "# Calculate revenue per order (sum of paid order items)\n",
    "order_items_paid = (\n",
    "    order_items\n",
    "    .join(paid_orders, \"OrderID\", \"inner\")\n",
    "    .withColumn(\"Revenue\", col(\"Quantity\").cast(\"double\") * col(\"CurrentPrice\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "order_revenue = (\n",
    "    order_items_paid\n",
    "    .groupBy(\"OrderID\")\n",
    "    .agg(spark_sum(\"Revenue\").alias(\"Revenue\"))\n",
    ")\n",
    "\n",
    "# Join with Orders to get CustomerID and OrderDateKey\n",
    "fact_customer_order = (\n",
    "    orders\n",
    "    .join(order_revenue, \"OrderID\", \"inner\")\n",
    "    .select(\n",
    "        col(\"CustomerID\"),\n",
    "        col(\"OrderID\"),\n",
    "        date_format(col(\"CreatedAt\"), \"yyyyMMdd\").cast(\"int\").alias(\"OrderDateKey\"),\n",
    "        col(\"Revenue\").cast(\"double\")\n",
    "    )\n",
    ")\n",
    "fact_customer_order.write.jdbc(mysql_url, \"FactCustomerOrder\", mode=\"append\", properties=mysql_props)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
