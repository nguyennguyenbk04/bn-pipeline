{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327ddce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, dayofmonth, month, quarter, year, dayofweek, date_format, sum as spark_sum\n",
    "\n",
    "# Set environment vars to load jars\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--jars jars/hadoop-azure-3.3.6.jar,\" # Hadoop connector (wasb + wasbs)\n",
    "    \"jars/mysql-connector-j-9.3.0.jar,\" # Allow write to mysql DB\n",
    "    \"jars/azure-storage-8.6.6.jar,\" # Azure SDK for Java (allow communication between Hadoop, Spark with Blobs Storage)\n",
    "    \"jars/jetty-client-9.4.43.v20210629.jar,\" # I don't know...\n",
    "    \"jars/jetty-http-9.4.43.v20210629.jar,\" ###############\n",
    "    \"jars/jetty-io-9.4.43.v20210629.jar,\" #################\n",
    "    \"jars/jetty-util-9.4.43.v20210629.jar,\" ################\n",
    "    \"jars/jetty-util-ajax-9.4.43.v20210629.jar \" ############\n",
    "    \"pyspark-shell\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca086e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/09 08:41:59 WARN Utils: Your hostname, lenovo-slim, resolves to a loopback address: 127.0.1.1; using 192.168.199.13 instead (on interface wlp2s0)\n",
      "25/07/09 08:41:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/09 08:42:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/09 08:42:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Init spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DW data load\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be60a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_url = \"jdbc:mysql://localhost:3306/store_dw\"\n",
    "mysql_props = {\n",
    "    \"user\": \"bnguyen\",\n",
    "    \"password\": \".Tldccmcbtldck2\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aae5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver access key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mysilver.blob.core.windows.net\",\n",
    "    \"bAthp0pVBfqEtyCvJElSX7MeI7ejSLa6cjuPoMz0Gg/69uzEW01y4URMDXsdFCrkpc9M54cDHnXs+AStj1gExQ==\"\n",
    ")\n",
    "\n",
    "# Gold\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mygold.dfs.core.windows.net\",\n",
    "    \"wRPXTwWCVxWwUpavEh62A5wzLdUvRTGeB3tZKP3eRbig7ca8ZN51l0kWS32kcbH/ddQ/jNXBzqDC+AStOzXlyw==\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384da969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/09 08:42:45 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-azure-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+-----+----------+--------+-------------------+-------------------+\n",
      "|ProductID|                Name|         Description|Price|CategoryID|SellerID|          CreatedAt|          UpdatedAt|\n",
      "+---------+--------------------+--------------------+-----+----------+--------+-------------------+-------------------+\n",
      "|        1|         Lamp AS-586|A decorative lamp...|  402|         3|    2320|2025-01-14 21:04:24|2025-01-19 19:49:46|\n",
      "|        2|       Tablet sp-018|A lightweight tab...|  378|         1|    3708|2023-11-14 09:02:40|2025-05-07 04:33:14|\n",
      "|        3|     Lipstick CB-813|A long-lasting li...|   63|         5|    3100|2024-04-30 17:20:16|2024-10-14 09:24:58|\n",
      "|        4|     Notebook jo-312|A ruled notebook ...|  147|        10|    2919|2024-12-10 10:14:21|2025-04-15 17:06:53|\n",
      "|        5|      T-Shirt oh-467|A comfortable cot...|   94|         2|     692|2025-01-24 01:21:32|2024-08-08 15:03:21|\n",
      "|        6|     Lipstick nH-281|A long-lasting li...|  199|         5|    2765|2024-03-03 22:37:08|2025-02-25 22:32:12|\n",
      "|        7|  Garden Hose Yu-412|A flexible garden...|  489|         9|    1217|2025-04-15 04:06:51|2024-09-08 13:53:29|\n",
      "|        8|     Curtains UB-587|Elegant curtains ...|  112|         3|    2961|2024-09-19 04:24:43|2024-11-16 15:41:19|\n",
      "|        9|GPS Navigator pG-904|A GPS navigator w...|  436|         7|     245|2024-03-08 17:05:05|2024-12-24 07:50:41|\n",
      "|       10|   Face Cream nL-672|A moisturizing fa...|  346|         5|    1972|2024-07-20 05:45:41|2024-07-22 10:24:01|\n",
      "|       11|Tennis Racket Fe-891|A lightweight ten...|  430|         4|    2874|2024-08-20 05:58:30|2024-10-28 12:42:57|\n",
      "|       12|   Car Vacuum dG-829|A portable vacuum...|  127|         7|    2953|2023-08-17 15:56:28|2024-11-28 11:28:51|\n",
      "|       13|        Dress rZ-583|A fashionable dre...|  307|         2|    2129|2024-12-23 19:09:46|2024-12-04 06:16:12|\n",
      "|       14|    Desk Lamp fr-180|A bright desk lam...|  492|        10|    1987|2023-12-06 02:37:55|2025-05-20 00:36:08|\n",
      "|       15|      Pen Set FP-931|A set of smooth-w...|  456|        10|    2940|2025-03-15 16:00:20|2025-03-27 12:38:56|\n",
      "|       16|        Jeans Hj-427|Stylish denim jea...|  469|         2|    1493|2023-07-29 07:17:29|2024-11-24 21:53:01|\n",
      "|       17|      Toy Car NG-311|A remote-controll...|  470|         6|     619|2024-12-08 00:52:05|2024-11-03 01:33:08|\n",
      "|       18| Dining Table Rg-965|A wooden dining t...|  243|         3|    1067|2024-03-19 21:28:58|2025-03-11 10:04:58|\n",
      "|       19| Dining Table CL-908|A wooden dining t...|   20|         3|    2279|2023-11-21 23:48:48|2025-03-21 14:38:50|\n",
      "|       20| Travel Guide rw-173|A guidebook for w...|   97|         8|    3762|2023-10-06 09:59:14|2025-03-17 00:07:16|\n",
      "+---------+--------------------+--------------------+-----+----------+--------+-------------------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Products\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d25fc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. DimProduct\n",
    "products = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Products\")\n",
    "dim_product = products.select(\n",
    "    col(\"ProductID\"),\n",
    "    col(\"Name\").alias(\"ProductName\"),\n",
    "    col(\"CategoryID\"),\n",
    "    col(\"SellerID\")\n",
    ")\n",
    "dim_product.write.jdbc(mysql_url, \"DimProduct\", mode=\"append\", properties=mysql_props)\n",
    "dim_product.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimProduct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e118e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2. DimCategory\n",
    "categories = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/ProductCategories\")\n",
    "dim_category = categories.select(\n",
    "    col(\"CategoryID\"),\n",
    "    col(\"CategoryName\")\n",
    ")\n",
    "dim_category.write.jdbc(mysql_url, \"DimCategory\", mode=\"append\", properties=mysql_props)\n",
    "dim_category.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimCategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7551ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3. DimSeller\n",
    "sellers = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Sellers\")\n",
    "dim_seller = sellers.select(\n",
    "    col(\"SellerID\"),\n",
    "    col(\"Name\").alias(\"SellerName\")\n",
    ")\n",
    "dim_seller.write.jdbc(mysql_url, \"DimSeller\", mode=\"append\", properties=mysql_props)\n",
    "dim_seller.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimSeller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "349af045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 4. DimCustomer\n",
    "customers = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Customers\")\n",
    "dim_customer = customers.select(\n",
    "    col(\"CustomerID\"),\n",
    "    col(\"Name\").alias(\"CustomerName\")\n",
    ")\n",
    "dim_customer.write.jdbc(mysql_url, \"DimCustomer\", mode=\"append\", properties=mysql_props)\n",
    "dim_customer.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimCustomer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77be3932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 5. DimOrderStatus\n",
    "order_status = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/OrderStatus\")\n",
    "dim_order_status = order_status.select(\n",
    "    col(\"StatusID\"),\n",
    "    col(\"StatusName\")\n",
    ")\n",
    "dim_order_status.write.jdbc(mysql_url, \"DimOrderStatus\", mode=\"append\", properties=mysql_props)\n",
    "dim_order_status.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimOrderStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e5bc0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 6. DimDate (from Orders)\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\")\n",
    "dim_date = orders.select(\n",
    "    date_format(col(\"CreatedAt\"), \"yyyyMMdd\").cast(\"int\").alias(\"DateKey\"),\n",
    "    col(\"CreatedAt\").cast(\"date\").alias(\"Date\"),\n",
    "    dayofmonth(col(\"CreatedAt\")).alias(\"Day\"),\n",
    "    month(col(\"CreatedAt\")).alias(\"Month\"),\n",
    "    quarter(col(\"CreatedAt\")).alias(\"Quarter\"),\n",
    "    year(col(\"CreatedAt\")).alias(\"Year\"),\n",
    "    dayofweek(col(\"CreatedAt\")).alias(\"DayOfWeek\")\n",
    ").distinct()\n",
    "dim_date.write.jdbc(mysql_url, \"DimDate\", mode=\"append\", properties=mysql_props)\n",
    "dim_date.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2f4b9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 7. DimReason\n",
    "reasons = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Reasons\")\n",
    "dim_reason = reasons.select(\n",
    "    col(\"ReasonID\"),\n",
    "    col(\"ReasonType\"),\n",
    "    col(\"ReasonDescription\")\n",
    ")\n",
    "dim_reason.write.jdbc(mysql_url, \"DimReason\", mode=\"append\", properties=mysql_props)\n",
    "dim_reason.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimReason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb7e758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fact sales\n",
    "order_items = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/OrderItems\").alias(\"oi\")\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\").alias(\"o\")\n",
    "payments = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Payments\").alias(\"p\")\n",
    "products = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Products\").alias(\"pr\")\n",
    "\n",
    "# Only consider orders that have a payment\n",
    "paid_orders = payments.select(\"OrderID\").distinct().alias(\"po\")\n",
    "\n",
    "# Join OrderItems with Orders and filter for paid orders\n",
    "fact_sales = (\n",
    "    order_items\n",
    "    .join(orders, col(\"oi.OrderID\") == col(\"o.OrderID\"))\n",
    "    .join(paid_orders, col(\"oi.OrderID\") == col(\"po.OrderID\"), \"inner\")\n",
    "    .join(products, col(\"oi.ProductID\") == col(\"pr.ProductID\"))\n",
    "    .join(payments, col(\"oi.OrderID\") == col(\"p.OrderID\"), \"inner\")  # Join with payments\n",
    "    .select(\n",
    "        col(\"oi.OrderItemID\"),\n",
    "        col(\"oi.OrderID\"),\n",
    "        col(\"oi.ProductID\"),\n",
    "        col(\"pr.SellerID\"),\n",
    "        col(\"o.CustomerID\"),\n",
    "        col(\"pr.CategoryID\"),\n",
    "        date_format(col(\"o.CreatedAt\"), \"yyyyMMdd\").cast(\"int\").alias(\"OrderDateKey\"),\n",
    "        col(\"o.StatusID\"),\n",
    "        col(\"oi.Quantity\").cast(\"int\").alias(\"Quantity\"),\n",
    "        col(\"oi.CurrentPrice\").cast(\"double\").alias(\"CurrentPrice\"),\n",
    "        (col(\"oi.Quantity\").cast(\"int\") * col(\"oi.CurrentPrice\").cast(\"double\")).alias(\"Revenue\"),\n",
    "        col(\"p.CreatedAt\").alias(\"CreatedAt\")  # Use payment time\n",
    "    )\n",
    ")\n",
    "fact_sales.write.jdbc(mysql_url, \"FactSales\", mode=\"append\", properties=mysql_props)\n",
    "fact_sales.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/FactSales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "803bbfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fact reasons\n",
    "reasons = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Reasons\")\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\")\n",
    "\n",
    "# Join Reasons with Orders to get OrderDateKey and StatusID\n",
    "fact_order_reason = (\n",
    "    reasons\n",
    "    .join(orders, reasons.OrderID == orders.OrderID, \"inner\")\n",
    "    .select(\n",
    "        reasons.ReasonID,\n",
    "        reasons.OrderID,\n",
    "        date_format(orders.CreatedAt, \"yyyyMMdd\").cast(\"int\").alias(\"OrderDateKey\"),\n",
    "        orders.StatusID,\n",
    "        reasons.CreatedAt.alias(\"CreatedAt\")  # Use Reasons.CreatedAt\n",
    "    )\n",
    ")\n",
    "fact_order_reason.write.jdbc(mysql_url, \"FactOrderReason\", mode=\"append\", properties=mysql_props)\n",
    "fact_order_reason.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/FactOrderReason\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
