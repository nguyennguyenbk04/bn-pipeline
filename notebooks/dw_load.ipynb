{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327ddce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt\n",
    "# python -m ipykernel install --user --name=python3\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, dayofmonth, month, quarter, year, dayofweek, date_format, sum as spark_sum\n",
    "\n",
    "jar_dir = \"/home/bnguyen/Desktop/DE_project/notebooks/jars\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    f\"--jars {jar_dir}/hadoop-azure-3.3.6.jar,\"\n",
    "    f\"{jar_dir}/azure-storage-8.6.6.jar,\"\n",
    "    f\"{jar_dir}/jetty-client-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-http-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-io-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/mysql-connector-j-9.3.0.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-9.4.43.v20210629.jar,\"\n",
    "    f\"{jar_dir}/jetty-util-ajax-9.4.43.v20210629.jar \"\n",
    "    \"pyspark-shell\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca086e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/09 11:17:53 WARN Utils: Your hostname, lenovo-slim, resolves to a loopback address: 127.0.1.1; using 192.168.199.13 instead (on interface wlp2s0)\n",
      "25/07/09 11:17:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/09 11:18:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Init spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DW data load\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be60a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_url = \"jdbc:mysql://localhost:3306/store_dw\"\n",
    "mysql_props = {\n",
    "    \"user\": \"bnguyen\",\n",
    "    \"password\": \".Tldccmcbtldck2\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aae5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver access key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mysilver.blob.core.windows.net\",\n",
    "    \"bAthp0pVBfqEtyCvJElSX7MeI7ejSLa6cjuPoMz0Gg/69uzEW01y4URMDXsdFCrkpc9M54cDHnXs+AStj1gExQ==\"\n",
    ")\n",
    "\n",
    "# Gold\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mygold.dfs.core.windows.net\",\n",
    "    \"wRPXTwWCVxWwUpavEh62A5wzLdUvRTGeB3tZKP3eRbig7ca8ZN51l0kWS32kcbH/ddQ/jNXBzqDC+AStOzXlyw==\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25fc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/09 11:18:11 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-azure-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. DimProduct\n",
    "products = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Products\")\n",
    "dim_product = products.select(\n",
    "    col(\"ProductID\"),\n",
    "    col(\"Name\").alias(\"ProductName\"),\n",
    "    col(\"CategoryID\"),\n",
    "    col(\"SellerID\")\n",
    ")\n",
    "dim_product.write.jdbc(mysql_url, \"DimProduct\", mode=\"overwrite\", properties=mysql_props)\n",
    "# dim_product.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimProduct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e118e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2. DimCategory\n",
    "categories = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/ProductCategories\")\n",
    "dim_category = categories.select(\n",
    "    col(\"CategoryID\"),\n",
    "    col(\"CategoryName\")\n",
    ")\n",
    "dim_category.write.jdbc(mysql_url, \"DimCategory\", mode=\"overwrite\", properties=mysql_props)\n",
    "# dim_category.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimCategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3. DimSeller\n",
    "sellers = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Sellers\")\n",
    "dim_seller = sellers.select(\n",
    "    col(\"SellerID\"),\n",
    "    col(\"Name\").alias(\"SellerName\")\n",
    ")\n",
    "dim_seller.write.jdbc(mysql_url, \"DimSeller\", mode=\"overwrite\", properties=mysql_props)\n",
    "# dim_seller.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimSeller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349af045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 4. DimCustomer\n",
    "customers = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Customers\")\n",
    "dim_customer = customers.select(\n",
    "    col(\"CustomerID\"),\n",
    "    col(\"Name\").alias(\"CustomerName\")\n",
    ")\n",
    "dim_customer.write.jdbc(mysql_url, \"DimCustomer\", mode=\"overwrite\", properties=mysql_props)\n",
    "# dim_customer.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimCustomer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be3932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 5. DimOrderStatus\n",
    "order_status = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/OrderStatus\")\n",
    "dim_order_status = order_status.select(\n",
    "    col(\"StatusID\"),\n",
    "    col(\"StatusName\")\n",
    ")\n",
    "dim_order_status.write.jdbc(mysql_url, \"DimOrderStatus\", mode=\"overwrite\", properties=mysql_props)\n",
    "# dim_order_status.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimOrderStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5bc0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 6. DimDate (from Orders)\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\")\n",
    "dim_date = orders.select(\n",
    "    date_format(col(\"CreatedAt\"), \"yyyyMMdd\").cast(\"int\").alias(\"DateKey\"),\n",
    "    col(\"CreatedAt\").cast(\"date\").alias(\"Date\"),\n",
    "    dayofmonth(col(\"CreatedAt\")).alias(\"Day\"),\n",
    "    month(col(\"CreatedAt\")).alias(\"Month\"),\n",
    "    quarter(col(\"CreatedAt\")).alias(\"Quarter\"),\n",
    "    year(col(\"CreatedAt\")).alias(\"Year\"),\n",
    "    dayofweek(col(\"CreatedAt\")).alias(\"DayOfWeek\")\n",
    ").distinct()\n",
    "dim_date.write.jdbc(mysql_url, \"DimDate\", mode=\"overwrite\", properties=mysql_props)\n",
    "# dim_date.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4b9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 7. DimReason\n",
    "reasons = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Reasons\")\n",
    "dim_reason = reasons.select(\n",
    "    col(\"ReasonID\"),\n",
    "    col(\"ReasonType\"),\n",
    "    col(\"ReasonDescription\")\n",
    ")\n",
    "dim_reason.write.jdbc(mysql_url, \"DimReason\", mode=\"overwrite\", properties=mysql_props)\n",
    "# dim_reason.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/DimReason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fact sales\n",
    "order_items = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/OrderItems\").alias(\"oi\")\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\").alias(\"o\")\n",
    "payments = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Payments\").alias(\"p\")\n",
    "products = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Products\").alias(\"pr\")\n",
    "inventory = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Inventory\").alias(\"inv\")  # Add this line\n",
    "\n",
    "# Only consider orders that have a payment\n",
    "paid_orders = payments.select(\"OrderID\").distinct().alias(\"po\")\n",
    "\n",
    "# Join OrderItems with Orders and filter for paid orders\n",
    "fact_sales = (\n",
    "    order_items\n",
    "    .join(orders, col(\"oi.OrderID\") == col(\"o.OrderID\"))\n",
    "    .join(paid_orders, col(\"oi.OrderID\") == col(\"po.OrderID\"), \"inner\")\n",
    "    .join(products, col(\"oi.ProductID\") == col(\"pr.ProductID\"))\n",
    "    .join(inventory, col(\"oi.ProductID\") == col(\"inv.ProductID\"), \"left\")  # Join with Inventory for UnitCost\n",
    "    .join(payments, col(\"oi.OrderID\") == col(\"p.OrderID\"), \"inner\")  # Join with payments\n",
    "    .select(\n",
    "        col(\"oi.OrderItemID\"),\n",
    "        col(\"oi.OrderID\"),\n",
    "        col(\"oi.ProductID\"),\n",
    "        col(\"pr.SellerID\"),\n",
    "        col(\"o.CustomerID\"),\n",
    "        col(\"pr.CategoryID\"),\n",
    "        date_format(col(\"o.CreatedAt\"), \"yyyyMMdd\").cast(\"int\").alias(\"OrderDateKey\"),\n",
    "        col(\"o.StatusID\"),\n",
    "        col(\"oi.Quantity\").cast(\"int\").alias(\"Quantity\"),\n",
    "        col(\"oi.CurrentPrice\").cast(\"double\").alias(\"CurrentPrice\"),\n",
    "        col(\"inv.UnitCost\").cast(\"double\").alias(\"UnitCost\"),  # New column\n",
    "        (col(\"oi.Quantity\").cast(\"int\") * col(\"oi.CurrentPrice\").cast(\"double\")).alias(\"Revenue\"),\n",
    "        (\n",
    "            (col(\"oi.Quantity\").cast(\"int\") * col(\"oi.CurrentPrice\").cast(\"double\")) -\n",
    "            (col(\"oi.Quantity\").cast(\"int\") * col(\"inv.UnitCost\").cast(\"double\"))\n",
    "        ).alias(\"Profit\"),  # New column\n",
    "        col(\"p.CreatedAt\").alias(\"CreatedAt\")  # Use payment time\n",
    "    )\n",
    ")\n",
    "fact_sales.write.jdbc(mysql_url, \"FactSales\", mode=\"overwrite\", properties=mysql_props)\n",
    "# fact_sales.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/FactSales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803bbfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fact reasons\n",
    "reasons = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Reasons\")\n",
    "orders = spark.read.parquet(\"wasbs://silver@mysilver.blob.core.windows.net/Orders\")\n",
    "\n",
    "# Join Reasons with Orders to get OrderDateKey and StatusID\n",
    "fact_order_reason = (\n",
    "    reasons\n",
    "    .join(orders, reasons.OrderID == orders.OrderID, \"inner\")\n",
    "    .select(\n",
    "        reasons.ReasonID,\n",
    "        reasons.OrderID,\n",
    "        date_format(orders.CreatedAt, \"yyyyMMdd\").cast(\"int\").alias(\"OrderDateKey\"),\n",
    "        orders.StatusID,\n",
    "        reasons.CreatedAt.alias(\"CreatedAt\")  # Use Reasons.CreatedAt\n",
    "    )\n",
    ")\n",
    "fact_order_reason.write.jdbc(mysql_url, \"FactOrderReason\", mode=\"append\", properties=mysql_props)\n",
    "# fact_order_reason.write.mode(\"overwrite\").parquet(\"abfss://gold-test@mygold.dfs.core.windows.net/FactOrderReason\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
